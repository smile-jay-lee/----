 

本科毕业设计（论文）
开 题 报 告

题目： 面向轻量化定位与地图构建方法研究

学    院	信息科学与工程学院
专    业	计算机科学与技术
班    级	22计算机一班
学    号	226002618
学生姓名	李杰
指导教师	彭成斌
开题日期	2025年12月25日
面向轻量化定位与地图构建方法研究 开题报告
一、选题的背景与意义
	视觉-惯性SLAM（Visual-Inertial SLAM, VI-SLAM）是机器人与智能设备实现自主导航的核心技术之一。它通过融合相机和惯性测量单元（IMU）数据，克服单一传感器的不足，实现高精度的位姿估计与地图构建[1][2]。近年来，随着无人机、移动机器人以及增强现实/虚拟现实（AR/VR）等领域的快速发展，VI-SLAM技术的应用场景不断拓展，对算法的实时性、鲁棒性提出了更高要求[2]。例如，ETH Zurich的EuRoC MAV 数据集提供了同步的双目相机和高质量惯性数据，被广泛用于VI-SLAM系统评估[3]；德国图宾根大学发布的TUM-VI数据集则提供了精细的相机光度标定，进一步推动了VI-SLAM算法的研究和发展[3]。
	现有主流VI-SLAM算法（如VINS-Fusion、ORB-SLAM3等）在精度和鲁棒性方面表现优异，但往往伴随高计算复杂度。在资源受限的嵌入式平台（如单板计算机Jetson Nano）或边缘设备上，这类算法的实时运行面临挑战。Krško等（2025）在Jetson Nano上实现ORB-SLAM3演示了可行性，但也暴露出对计算资源敏感的问题[4]。此外，在动态复杂环境下，系统还需应对光照变化、运动模糊和快速场景变化等挑战。最近的研究表明，回环检测虽然可以显著提升定位精度，但也增加了计算负担[1]。因此，如何在保证精度与鲁棒性的前提下，减轻算法计算和存储开销，实现VI-SLAM的轻量化，是当前研究的热点和难点问题。
	本课题以复现并优化经典VI-SLAM框架为切入点，首先验证现有算法的性能，然后针对前端特征处理和后端优化流程进行精简和改进。研究内容紧扣轻量化主题：通过特征点剪枝、描述子压缩、稀疏优化等手段，降低系统对CPU和内存的需求，提升在嵌入式平台上的运行效率和实时性。该选题既符合智能机器人和无人系统发展的技术需求，又具备理论研究与工程实现的结合点，具有重要的学术意义和工程价值。
二、研究的基本内容与拟解决的主要问题
	本课题分为两个阶段开展工作。第一阶段重点复现与验证现有的VINS-Fusion系统，包括搭建软件环境、编译代码和运行示例。通过在EuRoC MAV和TUM-VI等公开数据集上的测试，复现VINS-Fusion的定位轨迹与建图效果，记录其定位精度和运行时性能，为后续优化提供基线数据。VINS-Fusion作为图优化基础的VI-SLAM算法，支持单目/双目+IMU，多传感器融合以及回环检测等功能[5]。然而其计算量较大，尤其是在循环闭环、全局优化等模块上，对有限资源平台不友好。复现过程中拟解决的问题包括：系统集成和配置问题、代码依赖与编译问题，以及算法在不同场景下的稳定性验证。
	第二阶段基于OpenVINS框架展开轻量化优化研究。OpenVINS（Geneva等2020年提出）是一个开源的视觉惯性估计平台，采用扩展卡尔曼滤波（EKF）为核心，具有模块化和良好文档支持[2]。相比优化方法，滤波方法在计算上更轻量，但通常精度略低。本课题将结合两者特点：在OpenVINS的前端和后端框架上，设计特征点数量剪枝策略和描述子降维压缩方案，以减少每帧特征处理量；同时，探索基于稀疏优化的关键帧选择策略和局部窗口滑动优化，以降低后端优化复杂度。具体要解决的主要问题包括：如何动态调整前端提取的特征点数量以平衡精度与速度、如何对ORB描述子等特征描述进行压缩（如降维或二值量化）以节省带宽和存储、如何在滤波/优化框架中融合稀疏化策略以减少冗余状态估计计算。
实验验证方面，拟采用EuRoC MAV和TUM-VI数据集，通过标准评价指标（如绝对轨迹误差ATE、相对位姿误差RPE）对优化前后算法性能进行比较。同时，监测系统运行的CPU占用率和帧率，评估轻量化优化的实际效果。最终目标是在维持可接受的定位精度和地图一致性的前提下，大幅提升算法在资源受限环境下的实时性和稳定性，验证所提方法的可行性和有效性[1]。
三、研究的方法与技术路线
	研究方法上，本课题采用实验与开发并行推进的方式。首先在Ubuntu/Linux平台下搭建VINS-Fusion和OpenVINS运行环境，包括ROS或无ROS版本部署，确保两个系统能够正确处理EuRoC MAV和TUM-VI的传感器数据。通过分析VINS-Fusion的代码结构，理解其滤波与图优化的工作流程以及各模块功能，为剪枝优化提供依据。针对OpenVINS，用其完整功能作为基础，重点在滤波器参数设置、特征提取流程以及优化窗口管理上进行分析。
在具体技术路线方面，首先进行基线评测：运行原始VINS-Fusion与OpenVINS算法，记录定位精度、运行速度和资源占用，以确定优化方向。然后根据特点设计优化措施：前端优化包括限制每帧最大提取特征点数、采用加速特征检测算法或者ROI（感兴趣区域）裁剪；描述子优化方面可能采用主成分分析（PCA）等方法压缩描述子维度，或者使用更短位的二进制描述子；后端优化包括使用固定滑动窗口优化替代全局优化，或者通过信息矩阵稀疏化来减少非零元素。必要时，还可以借助OpenCV和Eigen等库自编写更高效的数据处理函数。
为模拟资源受限环境，可选采用Jetson Nano硬件测试或使用Docker容器模拟限算力平台。采用Gazebo或实时ROS回放技术，将算法部署在Jetson Nano平台，并使用On-chip GPU或CPU运行负载。通过实验对比分析，评估优化前后算法的帧率提升和误差变化。整个过程中，会迭代调整策略，保持定位准确度和地图质量。同时，记录每项优化的效果，为定量分析提供数据。最终，汇总优化方案：特征剪枝比例与环境变化的关系、描述子维度与匹配性能的权衡、优化窗口长度与位姿误差的权衡等，形成系统的技术路线图。
四、研究的总体安排与进度
	研究计划预计4-5个月完成，主要任务分解与时间安排如下：
	1. 第1个月（准备与基线复现）：查阅文献与相关资料，熟悉VINS-Fusion和OpenVINS原理和代码结构；搭建开发环境，复现VINS-Fusion系统；完成EuRoC MAV、TUM-VI数据集下载及预处理；运行VINS-Fusion获取初步定位结果和资源消耗基线。
	2. 第2个月（算法分析与初步优化）：分析VINS-Fusion和OpenVINS性能瓶颈，设计前端特征剪枝和描述子压缩方案；在OpenVINS框架中实现基本的特征数量控制；使用EuRoC和TUM-VI进行参数调试和效果评估，获得第一版优化结果。
	3. 第3个月（进阶优化与迭代）：根据初步实验结果，优化描述子压缩算法并在OpenVINS集成；设计后端稀疏优化策略，如固定窗口BA或信息稀疏技术；对各优化策略进行单独与联合测试，比较其对精度和实时性的影响；调整实现细节以平衡精度与效率。
	4. 第4个月（平台测试与综合评估）：在Jetson Nano或Docker环境中部署优化后的系统，测试实际运行性能；使用EuRoC、TUM-VI数据集进行完整系统评测，包括ATE/RPE计算和帧率统计；与基线结果对比，验证轻量化效果；撰写实验报告和论文大纲。
	5. 第5个月（总结与答辩准备）：整理优化方案和实验数据，分析各项技术指标；完善开题报告和论文内容，准备答辩PPT；根据导师和同行反馈进行修改，最终形成可用于答辩的完整开题报告文档。
	各阶段均穿插阅读最新文献，并根据进度随时调整方案。阶段性成果将在小组讨论会上交流，以确保研究进展和质量。
五、主要参考文献
[1] Schmidt F, Blessing C, Enzweiler M, et al. Visual-inertial SLAM for unstructured outdoor environments: Benchmarking the benefits and computational costs of loop closing[J]. Journal of Field Robotics, 2025, 42: 3726-3747.
[2] Geneva P, Eckenhoff K, Lee W, et al. OpenVINS: A research platform for visual-inertial estimation[C]. IEEE International Conference on Robotics and Automation (ICRA), 2020: 4666-4672.
[3] Geneva P, Eckenhoff K, Lee W, et al. Supported datasets[EB/OL]. https://docs.openvins.com/gs-datasets.html.
[4] Krško J, Nemec D, Šimák V, et al. Implementation of visual odometry on Jetson Nano[J]. Sensors, 2025, 25(4): 1025.
[5] Sharafutdinov D, Griguletskii M, Kopanev P, et al. Comparison of modern open-source visual SLAM approaches[J]. Optical Memory and Neural Networks, 2020, 29(3): 223-233.
[6] Wang S, Hu Q, Zhang X, et al. LVID-SLAM: A lightweight visual-inertial SLAM for dynamic scenes based on semantic information[J]. Sensors, 2025, 25(13): 4117.
[7] Campos C, Elvira R, Rodríguez J J G, et al. ORB-SLAM3: An accurate open-source library for visual, visual-inertial, and multimap SLAM[J]. IEEE Transactions on Robotics, 2021, 37(6): 1874-1890.
[8] 王柯赛, 姚锡凡, 黄宇, 等. 动态环境下的视觉SLAM研究评述[J]. 机器人, 2021, 43(6): 715-732.
